{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0 10]\n",
      " [ 1 11]\n",
      " [ 2 12]\n",
      " [ 3 13]\n",
      " [ 4 14]\n",
      " [ 5 15]\n",
      " [ 6 16]\n",
      " [ 7 17]\n",
      " [ 8 18]\n",
      " [ 9 19]]\n",
      "\n",
      "(1, 5)\n",
      "(1, 5)\n",
      "[[ True  True  True  True False]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "A=np.arange(0,10)\n",
    "B=np.arange(10,20)\n",
    "A=A.reshape(10,1)\n",
    "B=B.reshape(10,1)\n",
    "\n",
    "\n",
    "\n",
    "C = np.hstack((A,B))\n",
    "# C = torch.cat([A,B],dim=0) # 행방향으로 합침. 단 np가 torch로 바뀌어야 됨\n",
    "\n",
    "print(C)\n",
    "\n",
    "cond_domain1_heat_bc = np.matrix([0,1,2,3,4])\n",
    "#id1 = domain1_point_all[cond_domain1_heat_bc,1] <=13.5 \n",
    "\n",
    "id1 = C[cond_domain1_heat_bc,1] <=13.5 \n",
    "\n",
    "print()\n",
    "print(id1.shape)\n",
    "print(cond_domain1_heat_bc.shape)\n",
    "\n",
    "print(id1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function mean in module torch:\n",
      "\n",
      "mean(...)\n",
      "    mean(input, *, dtype=None) -> Tensor\n",
      "    \n",
      "    Returns the mean value of all elements in the :attr:`input` tensor.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the input tensor.\n",
      "    \n",
      "    Keyword args:\n",
      "        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "            If specified, the input tensor is casted to :attr:`dtype` before the operation\n",
      "            is performed. This is useful for preventing data type overflows. Default: None.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> a = torch.randn(1, 3)\n",
      "        >>> a\n",
      "        tensor([[ 0.2294, -0.5481,  1.3288]])\n",
      "        >>> torch.mean(a)\n",
      "        tensor(0.3367)\n",
      "    \n",
      "    .. function:: mean(input, dim, keepdim=False, *, dtype=None, out=None) -> Tensor\n",
      "       :noindex:\n",
      "    \n",
      "    Returns the mean value of each row of the :attr:`input` tensor in the given\n",
      "    dimension :attr:`dim`. If :attr:`dim` is a list of dimensions,\n",
      "    reduce over all of them.\n",
      "    \n",
      "    \n",
      "    If :attr:`keepdim` is ``True``, the output tensor is of the same size\n",
      "    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1.\n",
      "    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the\n",
      "    output tensor having 1 (or ``len(dim)``) fewer dimension(s).\n",
      "    \n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the input tensor.\n",
      "        dim (int or tuple of ints): the dimension or dimensions to reduce.\n",
      "        keepdim (bool): whether the output tensor has :attr:`dim` retained or not.\n",
      "    \n",
      "    Keyword args:\n",
      "        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "            If specified, the input tensor is casted to :attr:`dtype` before the operation\n",
      "            is performed. This is useful for preventing data type overflows. Default: None.\n",
      "        out (Tensor, optional): the output tensor.\n",
      "    \n",
      "    .. seealso::\n",
      "    \n",
      "        :func:`torch.nanmean` computes the mean value of `non-NaN` elements.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> a = torch.randn(4, 4)\n",
      "        >>> a\n",
      "        tensor([[-0.3841,  0.6320,  0.4254, -0.7384],\n",
      "                [-0.9644,  1.0131, -0.6549, -1.4279],\n",
      "                [-0.2951, -1.3350, -0.7694,  0.5600],\n",
      "                [ 1.0842, -0.9580,  0.3623,  0.2343]])\n",
      "        >>> torch.mean(a, 1)\n",
      "        tensor([-0.0163, -0.5085, -0.4599,  0.1807])\n",
      "        >>> torch.mean(a, 1, True)\n",
      "        tensor([[-0.0163],\n",
      "                [-0.5085],\n",
      "                [-0.4599],\n",
      "                [ 0.1807]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10 11 12 13]]\n"
     ]
    }
   ],
   "source": [
    "#q1 = C[cond_domain1_heat_bc[id1],1] * 10/13.5\n",
    "q1 = C[cond_domain1_heat_bc[id1],1]\n",
    "\n",
    "print(q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain x, t를 만든다!\n",
    "domain1_x = np.linspace(0, 2.5, 21)\n",
    "domain1_t = np.arange(0, 27+0.1, 0.1)\n",
    "x, t = np.meshgrid(domain1_x, domain1_t)\n",
    "domain1_grid = np.hstack((x.flatten()[:,None], t.flatten()[:,None]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40,)\n",
      "(40, 1)\n"
     ]
    }
   ],
   "source": [
    "print((x.flatten())[0:40].shape)\n",
    "AAAA = (x.flatten())[0:40, None]\n",
    "print(AAAA.shape)\n",
    "\n",
    "#print((t.flatten())[0:40, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5691, 2)\n",
      "(5691, 2)\n"
     ]
    }
   ],
   "source": [
    "point_all = domain1_grid\n",
    "indexes  = np.unique(point_all, axis=0, return_index=True)[1]   \n",
    "point_all2 = point_all[np.sort(indexes),:]   \n",
    "x = point_all[np.sort(indexes),:]   \n",
    "print(point_all2.shape)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''        point 구분        '''\n",
    "domain1_point_x = torch.tensor(x[:,0].reshape(-1,1)).float()    # x 값만 다시 받아옴\n",
    "domain1_point_t = torch.tensor(x[:,1].reshape(-1,1)).float()    # t 값만 다시 받아옴\n",
    "\n",
    "# 240622 김영중 수정\n",
    "domain1_point_all = torch.cat([domain1_point_x, domain1_point_t], dim=1)    # domain1_point_x와 domain1_point_t를 옆으로 쌓아? 그럼 다시 X 형태의 (5691,2) 인거 같은데?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min-max 정규화를 이렇게 직접 해줬음\n",
    "min_input_x = np.min(x[:,0])\n",
    "max_input_x = np.max(x[:,0])\n",
    "\n",
    "min_input_t = np.min(x[:,1])\n",
    "max_input_t = np.max(x[:,1])\n",
    "\n",
    "min_output = 40\n",
    "max_output = 100\n",
    "\n",
    "# 정규화 과정에서 붙음 coefficient 같은거\n",
    "cc_xx = (max_output-min_output) / (max_input_x-min_input_x)**2\n",
    "cc_x = (max_output-min_output) / (max_input_x-min_input_x)\n",
    "cc_t = (max_output-min_output) / (max_input_t-min_input_t)\n",
    "\n",
    "input_x_normalized = (x[:,0].copy() - min_input_x) / (max_input_x - min_input_x)\n",
    "input_t_normalized = (x[:,1].copy() - min_input_t) / (max_input_t - min_input_t)\n",
    "\n",
    "# 어쨌든 input 은 normalize 했다는 뜻\n",
    "domain1_point_x_normal = torch.tensor(input_x_normalized.reshape(-1,1), requires_grad=True).float()  # domain 값을 normalized 시행\n",
    "domain1_point_t_normal = torch.tensor(input_t_normalized.reshape(-1,1), requires_grad=True).float()  # domain 값을 normalized 시행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on _ArrayFunctionDispatcher in module numpy:\n",
      "\n",
      "hstack(tup, *, dtype=None, casting='same_kind')\n",
      "    Stack arrays in sequence horizontally (column wise).\n",
      "    \n",
      "    This is equivalent to concatenation along the second axis, except for 1-D\n",
      "    arrays where it concatenates along the first axis. Rebuilds arrays divided\n",
      "    by `hsplit`.\n",
      "    \n",
      "    This function makes most sense for arrays with up to 3 dimensions. For\n",
      "    instance, for pixel-data with a height (first axis), width (second axis),\n",
      "    and r/g/b channels (third axis). The functions `concatenate`, `stack` and\n",
      "    `block` provide more general stacking and concatenation operations.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    tup : sequence of ndarrays\n",
      "        The arrays must have the same shape along all but the second axis,\n",
      "        except 1-D arrays which can be any length.\n",
      "    \n",
      "    dtype : str or dtype\n",
      "        If provided, the destination array will have this dtype. Cannot be\n",
      "        provided together with `out`.\n",
      "    \n",
      "    .. versionadded:: 1.24\n",
      "    \n",
      "    casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n",
      "        Controls what kind of data casting may occur. Defaults to 'same_kind'.\n",
      "    \n",
      "    .. versionadded:: 1.24\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    stacked : ndarray\n",
      "        The array formed by stacking the given arrays.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    concatenate : Join a sequence of arrays along an existing axis.\n",
      "    stack : Join a sequence of arrays along a new axis.\n",
      "    block : Assemble an nd-array from nested lists of blocks.\n",
      "    vstack : Stack arrays in sequence vertically (row wise).\n",
      "    dstack : Stack arrays in sequence depth wise (along third axis).\n",
      "    column_stack : Stack 1-D arrays as columns into a 2-D array.\n",
      "    hsplit : Split an array into multiple sub-arrays horizontally (column-wise).\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> a = np.array((1,2,3))\n",
      "    >>> b = np.array((4,5,6))\n",
      "    >>> np.hstack((a,b))\n",
      "    array([1, 2, 3, 4, 5, 6])\n",
      "    >>> a = np.array([[1],[2],[3]])\n",
      "    >>> b = np.array([[4],[5],[6]])\n",
      "    >>> np.hstack((a,b))\n",
      "    array([[1, 4],\n",
      "           [2, 5],\n",
      "           [3, 6]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "help(np.hstack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "'''        BC, IC index        '''\n",
    "cond_domain1_heat_bc = ( (domain1_point_all[:,0]==0) ).nonzero(as_tuple=True)[0]    # 여기 자세히 들여다봐야될듯 어떻게 나누고 있을까?240609\n",
    "cond_domain1_insul_bc = ( (domain1_point_all[:,0]==2.5)  ).nonzero(as_tuple=True)[0]\n",
    "cond_domain1_T0_IC = ( (domain1_point_all[:,1]==0)  ).nonzero(as_tuple=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    1    2 ... 5688 5689 5690]\n"
     ]
    }
   ],
   "source": [
    "kkkkk=np.sort(indexes)\n",
    "print(kkkkk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[1, 0, 0],\n",
      "       [2, 3, 4]]), array([0, 2], dtype=int64))\n",
      "(array([[0, 0, 1],\n",
      "       [0, 0, 1],\n",
      "       [3, 4, 2]]), array([1, 2, 0], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# unique는 그 요소 중 겹치는거 빼고, basis는 몇개로 구성되어 있냐는 뜻이고 axis는 그 방향으로 봤을때!\n",
    "# 아래 예제가 제대로 된 의미!\n",
    "\n",
    "a = np.array([[1, 0, 0],\n",
    "              [1, 0, 0],\n",
    "              [2, 3, 4]])\n",
    "\n",
    "print(np.unique(a, axis = 0, return_index=True))\n",
    "print(np.unique(a, axis = 1, return_index=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
